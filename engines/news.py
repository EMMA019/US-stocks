import json
import time
from pathlib import Path

import feedparser
import yfinance as yf

from config import CONFIG

CACHE_DIR = Path("./cache_v45")
CACHE_DIR.mkdir(exist_ok=True)

# BeautifulSoup4 ãŒä½¿ãˆã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ï¼‰
_BS4_AVAILABLE = False
try:
    import requests
    from bs4 import BeautifulSoup
    _BS4_AVAILABLE = True
except ImportError:
    pass

# ==============================================================================
# ğŸ“° NewsEngine
# ==============================================================================

class NewsEngine:
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ï¼‹æœ¬æ–‡æŠœç²‹ã‚’å–å¾—ã—ã¦AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨æ–‡å­—åˆ—ã‚’è¿”ã™ã€‚

    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ TTL = 1æ™‚é–“ï¼ˆã‚¹ã‚­ãƒ£ãƒ³é »åº¦ã«åˆã‚ã›ã¦æ›´æ–°ï¼‰
    """

    @staticmethod
    def get(ticker: str) -> dict:
        """
        Returns:
            {
                "articles": [{"title": str, "url": str, "body": str}, ...],
                "fetched_at": str
            }
        """
        cache_file = CACHE_DIR / f"news_{ticker}.json"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
        if cache_file.exists():
            expiry = CONFIG.get("NEWS_CACHE_EXPIRY", 3600)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ™‚é–“
            if time.time() - cache_file.stat().st_mtime < expiry:
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        return json.load(f)
                except Exception:
                    pass

        articles: list[dict] = []
        seen: set[str] = set()

        # â‘  Yahoo Finance ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæœ€å¤§5ä»¶ï¼‰
        try:
            ticker_obj = yf.Ticker(ticker)
            news_items = ticker_obj.news or []
            for n in news_items[:5]:
                title = n.get("title") or n.get("headline", "")
                url = n.get("link") or n.get("url", "")
                if title and title not in seen:
                    seen.add(title)
                    articles.append({"title": title, "url": url, "body": ""})
        except Exception:
            pass

        # â‘¡ Google News RSSï¼ˆç›´è¿‘3æ—¥ã€æœ€å¤§5ä»¶ï¼‰
        try:
            query = f"{ticker}+stock+when:3d"
            rss_url = (
                f"https://news.google.com/rss/search"
                f"?q={query}&hl=en-US&gl=US&ceid=US:en"
            )
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                title = entry.title
                if title not in seen:
                    seen.add(title)
                    articles.append({
                        "title": title,
                        "url": getattr(entry, "link", ""),
                        "body": "",
                    })
        except Exception:
            pass

        # â‘¢ æœ¬æ–‡æŠœç²‹å–å¾—ï¼ˆä¸Šä½3ä»¶ã¾ã§ã€BS4ãŒä½¿ãˆã‚‹å ´åˆï¼‰
        if _BS4_AVAILABLE:
            timeout = CONFIG.get("NEWS_FETCH_TIMEOUT", 8)      # ç§’
            max_chars = CONFIG.get("NEWS_MAX_CHARS", 800)

            for art in articles[:3]:
                url = art.get("url")
                if not url:
                    continue
                try:
                    r = requests.get(
                        url,
                        headers={"User-Agent": "Mozilla/5.0 (compatible; NewsBot/1.0)"},
                        timeout=timeout,
                    )
                    r.raise_for_status()
                    soup = BeautifulSoup(r.text, "html.parser")

                    # é•·ã„æ®µè½ã®ã¿æŠ½å‡º
                    paras = [
                        p.get_text().strip()
                        for p in soup.find_all("p")
                        if len(p.get_text().strip()) > 50
                    ]
                    body_text = " ".join(paras)[:max_chars]
                    art["body"] = body_text
                except Exception:
                    art["body"] = ""  # å¤±æ•—ã—ã¦ã‚‚ç©ºæ–‡å­—ã§ç¶™ç¶š

        result = {
            "articles": articles[:8],  # æœ€å¤§8ä»¶ã«åˆ¶é™
            "fetched_at": time.strftime("%Y-%m-%d %H:%M"),
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)

        return result

    @staticmethod
    def format_for_prompt(news: dict) -> str:
        """
        AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ¸¡ã™æ–‡å­—åˆ—ã‚’ç”Ÿæˆã€‚
        æœ¬æ–‡æŠœç²‹ãŒã‚ã‚‹è¨˜äº‹ã¯è¦‹å‡ºã—ï¼‹æŠœç²‹ã‚’ã€ãªã„è¨˜äº‹ã¯è¦‹å‡ºã—ã®ã¿å‡ºåŠ›ã€‚
        """
        lines: list[str] = []
        articles = news.get("articles", [])

        if not articles:
            return "æœ¬æ—¥ã€æ–°è¦ææ–™ã¯æœªæ¤œå‡ºã€‚"

        for a in articles:
            lines.append(f"â€¢ {a['title']}")
            body = a.get("body", "").strip()
            if body:
                # æœ€åˆã®200æ–‡å­—ç¨‹åº¦ã«çŸ­ç¸®ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ç¯€ç´„ï¼‰
                excerpt = body[:200] + ("..." if len(body) > 200 else "")
                lines.append(f"  æŠœç²‹: {excerpt}")

        return "\n".join(lines)

    # ========== æ–°ã—ãè¿½åŠ  ==========
    @staticmethod
    def get_general_market() -> dict:
        """
        å¸‚å ´å…¨ä½“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹ï¼ˆSPYã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä»£è¡¨ã¨ã—ã¦ä½¿ç”¨ï¼‰
        """
        return NewsEngine.get("SPY")