import json
import time
from pathlib import Path

import feedparser
import yfinance as yf

from config import CONFIG

CACHE_DIR = Path(â€./cache_v45â€)
CACHE_DIR.mkdir(exist_ok=True)

# BeautifulSoup4 ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãªã‚‰æœ¬æ–‡fetchã‚’æœ‰åŠ¹åŒ–ï¼‰

try:
import requests
from bs4 import BeautifulSoup
_BS4_AVAILABLE = True
except ImportError:
_BS4_AVAILABLE = False

# ==============================================================================

# ğŸ“° NewsEngine

# ==============================================================================

class NewsEngine:
â€œâ€â€
ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ï¼‹æœ¬æ–‡æŠœç²‹ã‚’å–å¾—ã—ã¦AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨æ–‡å­—åˆ—ã‚’è¿”ã™ã€‚

```
ã‚­ãƒ£ãƒƒã‚·ãƒ¥ TTL = 1æ™‚é–“ï¼ˆã‚¹ã‚­ãƒ£ãƒ³é »åº¦ã«åˆã‚ã›ã¦æ›´æ–°ï¼‰
"""

@staticmethod
def get(ticker: str) -> dict:
    """
    Returns:
        {
            "articles": [{"title": str, "url": str, "body": str}, ...],
            "fetched_at": str
        }
    """
    cache_file = CACHE_DIR / f"news_{ticker}.json"

    if cache_file.exists():
        if time.time() - cache_file.stat().st_mtime < CONFIG["NEWS_CACHE_EXPIRY"]:
            try:
                with open(cache_file) as f:
                    return json.load(f)
            except:
                pass

    articles: list[dict] = []
    seen: set[str] = set()

    # â‘  Yahoo Finance ãƒ‹ãƒ¥ãƒ¼ã‚¹
    try:
        for n in (yf.Ticker(ticker).news or [])[:5]:
            title = n.get("title", n.get("headline", ""))
            url   = n.get("link",  n.get("url", ""))
            if title and title not in seen:
                seen.add(title)
                articles.append({"title": title, "url": url, "body": ""})
    except:
        pass

    # â‘¡ Google News RSSï¼ˆç›´è¿‘3æ—¥ï¼‰
    try:
        feed = feedparser.parse(
            f"https://news.google.com/rss/search"
            f"?q={ticker}+stock+when:3d&hl=en-US&gl=US&ceid=US:en"
        )
        for entry in feed.entries[:5]:
            if entry.title not in seen:
                seen.add(entry.title)
                articles.append({
                    "title": entry.title,
                    "url":   getattr(entry, "link", ""),
                    "body":  "",
                })
    except:
        pass

    # â‘¢ æœ¬æ–‡fetchï¼ˆä¸Šä½3è¨˜äº‹ï¼‰
    if _BS4_AVAILABLE:
        import requests as _req
        for art in articles[:3]:
            if not art["url"]:
                continue
            try:
                r = _req.get(
                    art["url"],
                    headers={"User-Agent": "Mozilla/5.0"},
                    timeout=CONFIG["NEWS_FETCH_TIMEOUT"],
                )
                soup  = BeautifulSoup(r.text, "html.parser")
                paras = [
                    p.get_text().strip()
                    for p in soup.find_all("p")
                    if len(p.get_text().strip()) > 50
                ]
                art["body"] = " ".join(paras)[:CONFIG["NEWS_MAX_CHARS"]]
            except:
                pass

    result = {
        "articles":   articles[:8],
        "fetched_at": time.strftime("%Y-%m-%d %H:%M"),
    }

    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False)

    return result

@staticmethod
def format_for_prompt(news: dict) -> str:
    """
    AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ¸¡ã™æ–‡å­—åˆ—ã‚’ç”Ÿæˆã€‚
    æœ¬æ–‡æŠœç²‹ãŒã‚ã‚‹è¨˜äº‹ã¯è¦‹å‡ºã—ï¼‹æŠœç²‹ã‚’ã€ãªã„è¨˜äº‹ã¯è¦‹å‡ºã—ã®ã¿å‡ºåŠ›ã€‚
    """
    lines: list[str] = []
    for a in news.get("articles", []):
        lines.append(f"â€¢ {a['title']}")
        if a.get("body"):
            lines.append(f"  æŠœç²‹: {a['body'][:200]}")
    return "\n".join(lines) if lines else "æœ¬æ—¥ã€æ–°è¦ææ–™ã¯æœªæ¤œå‡ºã€‚"
```